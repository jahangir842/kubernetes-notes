### Guide: Applying Rancher Local Path Provisioner

This guide walks you through setting up Rancher’s **Local Path Provisioner** on your Kubernetes cluster after installation. It assumes your cluster is running and you’re working from your workstation with `kubectl` configured.

---

#### **What is Rancher Local Path Provisioner?**
- A lightweight storage solution that dynamically provisions Persistent Volumes (PVs) using local storage on nodes.
- Uses the `local-path` StorageClass to create `hostPath`-based PVs when a Persistent Volume Claim (PVC) is requested.
- Ideal for bare-metal or simple setups needing local storage without external systems.

---

#### **Prerequisites**
- Kubernetes cluster with 1 master and 2 worker nodes, fully operational.
- Workstation with `kubectl` installed and configured (e.g., `~/.kube/config` copied from master).
- Network access from workstation to cluster.
- Worker nodes have writable directories (e.g., `/opt/local-path-provisioner`) for storage.

---

#### **Steps to Apply Local Path Provisioner**

1. **Verify Cluster Access from Workstation**
   - Run this on your workstation to confirm connectivity:
     ```bash
     kubectl get nodes
     ```
   - Expected output:
     ```
     NAME         STATUS   ROLES    AGE   VERSION
     master-node  Ready    master   1d    v1.29.0
     worker1      Ready    <none>   1d    v1.29.0
     worker2      Ready    <none>   1d    v1.29.0
     ```

2. **Deploy the Local Path Provisioner**
   - From your workstation, apply the official manifest:
     ```bash
     kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
     ```
   - This deploys:
     - A `local-path` StorageClass.
     - A Deployment with the `local-path-provisioner` pod in the `local-path-storage` namespace.
     - ConfigMaps and RBAC resources for operation.

3. **Check Deployment Status**
   - Verify the provisioner pod is running:
     ```bash
     kubectl get pods -n local-path-storage
     ```
   - Example output:
     ```
     NAME                              READY   STATUS    RESTARTS   AGE
     local-path-provisioner-abc123     1/1     Running   0          5m
     ```
   - Note: The pod typically runs on a worker node (`worker1` or `worker2`) due to the master’s `NoSchedule` taint.

4. **Prepare Worker Nodes for Storage**
   - The provisioner defaults to creating PVs in `/opt/local-path-provisioner` on the node where a pod is scheduled.
   - On each worker node (`worker1` and `worker2`), ensure this directory exists and is writable:
     ```bash
     ssh <worker1-user>@<worker1-ip> "sudo mkdir -p /opt/local-path-provisioner && sudo chmod 777 /opt/local-path-provisioner"
     ssh <worker2-user>@<worker2-ip> "sudo mkdir -p /opt/local-path-provisioner && sudo chmod 777 /opt/local-path-provisioner"
     ```
   - Replace `<worker1-user>`, `<worker1-ip>`, etc., with your actual credentials/IPs.

5. **Test with a Sample PVC**
   - Create a PVC to test the provisioner:
     ```yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: test-pvc
     spec:
       accessModes:
         - ReadWriteOnce
       resources:
         requests:
           storage: 1Gi
       storageClassName: local-path
     ```
   - Apply it from your workstation:
     ```bash
     kubectl apply -f pvc.yaml
     ```
   - Check the PV is created and bound:
     ```bash
     kubectl get pv
     kubectl get pvc
     ```

6. **Verify Storage Location**
   - Describe the PV to see which node it’s tied to:
     ```bash
     kubectl describe pv <pv-name>
     ```
   - Look for `nodeAffinity` to confirm it’s on a worker node (e.g., `worker1`).

---

#### **Where Does It Run?**
- **Deployment**: Applied cluster-wide from your workstation using `kubectl`.
- **Provisioner Pod**: Runs on one worker node (e.g., `worker1` or `worker2`), scheduled by Kubernetes. Avoids master due to its default taint (`node-role.kubernetes.io/master:NoSchedule`).
- **Storage**: Provisioned on the worker node where the PVC-using pod runs (e.g., `/opt/local-path-provisioner` on `worker2` if the pod is there).

---

#### **Customizing (Optional)**
- **Restrict to Specific Nodes**:
  - Edit the ConfigMap in `local-path-storage` to specify storage paths on `worker1` and `worker2`:
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: local-path-config
      namespace: local-path-storage
    data:
      setup: |
        nodePathMap:
        - node: worker1
          paths: ["/data/storage"]
        - node: worker2
          paths: ["/data/storage"]
    ```
    Apply it:
    ```bash
    kubectl apply -f configmap.yaml
    ```
  - Update worker nodes to match (e.g., `mkdir -p /data/storage`).

- **Allow Master Node (Not Recommended)**:
  - Remove the taint if you want the provisioner or storage on the master:
    ```bash
    kubectl taint nodes <master-node-name> node-role.kubernetes.io/master:NoSchedule-
    ```

---

#### **Troubleshooting**
- **Pod Not Running**: Check logs:
  ```bash
  kubectl logs -n local-path-storage <provisioner-pod-name>
  ```
- **PVC Stuck in Pending**: Ensure the `local-path` StorageClass exists (`kubectl get sc`) and worker nodes have the storage directory.
- **Storage Not Created**: Verify permissions on `/opt/local-path-provisioner` (or custom path).

---

#### **Next Steps**
- Use the `local-path` StorageClass in your applications (e.g., MLflow, databases) by specifying it in PVCs.
- Monitor PVs and PVCs with:
  ```bash
  kubectl get pv -o wide
  kubectl get pvc -o wide
  ```

---

#### **Summary**
After setting up your master and worker nodes, deploy the Local Path Provisioner from your workstation with `kubectl`. It runs as a pod on a worker node and provisions local storage on the node where your workloads are scheduled. Keep the master free for control plane tasks, prepare worker nodes for storage, and test with a PVC to confirm it’s working.

Let me know if you need help with a specific application setup!
