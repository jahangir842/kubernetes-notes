### **NFS Storage Provisioner**

- A storage provisioner that uses **Network File System (NFS)** to create Persistent Volumes (PVs) accessible over the network.
- Works with an NFS StorageClass to dynamically provision PVs for Persistent Volume Claims (PVCs).
- Ideal for clusters needing shared storage across multiple nodes, supporting `ReadWriteMany` (RWX) and `ReadWriteOnce` (RWO) access modes.
- Unlike local storage, NFS allows multiple pods on different nodes to mount the same volume simultaneously (for RWX), and it’s not tied to a single node’s filesystem.
- Supports manual resizing by adjusting the NFS server’s disk space, though dynamic resizing in Kubernetes requires additional configuration (`allowVolumeExpansion`).

---

#### **Where to Apply It**
- **Deployment**: Apply cluster-wide from your workstation using `kubectl`.
- **Runs On**: Requires an NFS server (can run on the master, a worker, or a separate machine) and an NFS provisioner pod (typically on a worker node).
- **Storage Location**: Provisions storage on the NFS server’s shared directory, accessible to all nodes in the cluster.

---

#### **Steps to Apply and Configure**

1. **Verify Cluster Access**
   - From your workstation, confirm you can see all nodes:
     ```bash
     kubectl get nodes
     ```
     Expected output:
     ```
     NAME         STATUS   ROLES    AGE   VERSION
     master-node  Ready    master   1d    v1.29.0
     worker1      Ready    <none>   1d    v1.29.0
     worker2      Ready    <none>   1d    v1.29.0
     ```

2. **Set Up an NFS Server**
   - Choose a node (e.g., `worker1`) or a separate machine to act as the NFS server.
   - Install NFS server software (example for Ubuntu):
     ```bash
     ssh <worker1-user>@<worker1-ip> "sudo apt update && sudo apt install -y nfs-kernel-server"
     ```
   - Create a shared directory (e.g., `/nfs/storage`):
     ```bash
     ssh <worker1-user>@<worker1-ip> "sudo mkdir -p /nfs/storage && sudo chmod 777 /nfs/storage"
     ```
   - Configure NFS exports by editing `/etc/exports`:
     ```bash
     ssh <worker1-user>@<worker1-ip> "echo '/nfs/storage *(rw,sync,no_subtree_check,no_root_squash)' | sudo tee -a /etc/exports"
     ```
   - Export and start NFS:
     ```bash
     ssh <worker1-user>@<worker1-ip> "sudo exportfs -ra && sudo systemctl restart nfs-kernel-server"
     ```
   - Note the NFS server IP (e.g., `<worker1-ip>`).

3. **Install NFS Client on All Nodes**
   - Ensure all nodes (master, `worker1`, `worker2`) can mount NFS shares:
     ```bash
     ssh <master-user>@<master-ip> "sudo apt install -y nfs-common"
     ssh <worker1-user>@<worker1-ip> "sudo apt install -y nfs-common"
     ssh <worker2-user>@<worker2-ip> "sudo apt install -y nfs-common"
     ```

4. **Deploy the NFS Provisioner**
   - Use an NFS provisioner like `nfs-subdir-external-provisioner`:
     ```bash
     kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/nfs-subdir-external-provisioner/master/deploy/deployment.yaml
     ```
   - Edit the deployment to set your NFS server details:
     ```bash
     kubectl edit deployment nfs-client-provisioner -n default
     ```
     Update the environment variables:
     ```yaml
     spec:
       template:
         spec:
           containers:
           - name: nfs-client-provisioner
             env:
             - name: NFS_SERVER
               value: "<worker1-ip>"  # Replace with your NFS server IP
             - name: NFS_PATH
               value: "/nfs/storage"  # Your NFS shared directory
     ```
   - Apply the StorageClass:
     ```yaml
     apiVersion: storage.k8s.io/v1
     kind: StorageClass
     metadata:
       name: nfs-client
     provisioner: cluster.local/nfs-client-provisioner  # Matches provisioner name
     parameters:
       archiveOnDelete: "false"  # Deletes PV data when PVC is removed
     reclaimPolicy: Delete
     ```
     ```bash
     kubectl apply -f storageclass.yaml
     ```

5. **Check Deployment**
   - Verify the provisioner pod is running (likely on a worker node):
     ```bash
     kubectl get pods -n default -l app=nfs-client-provisioner
     ```
     Example:
     ```
     NAME                                   READY   STATUS    RESTARTS   AGE
     nfs-client-provisioner-abc123          1/1     Running   0          5m
     ```

6. **Test the Provisioner**
   - Create a PVC using the `nfs-client` StorageClass:
     ```yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: test-pvc
       namespace: default
     spec:
       accessModes:
         - ReadWriteMany  # NFS supports RWX
       resources:
         requests:
           storage: 1Gi
       storageClassName: nfs-client
     ```
     Apply it:
     ```bash
     kubectl apply -f pvc.yaml
     ```
   - Verify PV creation:
     ```bash
     kubectl get pv -o wide
     ```
   - Check the NFS server (`<worker1-ip>`:/nfs/storage) for a subdirectory like `<pvc-uuid>`.

---

#### **Key Configuration Notes**
- **Master Node**: Can host the NFS server if needed, but it’s better on a worker or separate machine to avoid control plane load. Remove taint (`node-role.kubernetes.io/master:NoSchedule-`) if using master.
- **Worker Nodes**: Must have `nfs-common` installed to mount NFS shares. The provisioner pod runs on one worker.
- **Custom NFS Path**: Change `/nfs/storage` in the provisioner deployment or `/etc/exports` on the NFS server as needed.
- **Access Modes**: Use `ReadWriteMany` (RWX) for multi-node access or `ReadWriteOnce` (RWO) for single-node access—NFS supports both.

---

#### **Troubleshooting**
- **Pod Fails**: Check logs:
  ```bash
  kubectl logs -n default <provisioner-pod-name>
  ```
- **PVC Pending**: Verify NFS server is reachable (`ping <worker1-ip>`), `nfs-client` StorageClass exists (`kubectl get sc`), and NFS exports are correct.
- **Mount Issues**: Test NFS manually:
  ```bash
  ssh <worker2-user>@<worker2-ip> "sudo mount -t nfs <worker1-ip>:/nfs/storage /mnt"
  ```

---

#### **Summary**
- Apply the NFS Storage Provisioner from your workstation with `kubectl` to manage cluster-wide networked storage.
- Requires an NFS server (e.g., on `worker1`) and a provisioner pod (on a worker node) to dynamically create PVs from a shared NFS directory (e.g., `/nfs/storage`).
- Supports `ReadWriteMany`, making it suitable for multi-pod access, unlike `local-path`’s node-specific storage.
- Test with a PVC to ensure proper setup.

This NFS-based setup provides flexible, shared storage for your cluster, replacing the limitations of local storage with network accessibility! Let me know if you need help with resizing or specific NFS tweaks.