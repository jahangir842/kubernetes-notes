## Rancher Local Path Storage Provisioner

- A dynamic storage provisioner that creates Persistent Volumes (PVs) using local `hostPath` storage on nodes.
- Works with the `local-path` StorageClass to provision PVs automatically for Persistent Volume Claims (PVCs).
- Great for lightweight or bare-metal clusters without external storage systems.
- This provisioner doesn’t support dynamic resizing for hostPath-based PVs—it creates a fixed-size directory tied to the node’s filesystem (e.g., /data/storage/pvc-<uuid>).
- The local-path StorageClass doesn’t support volume expansion by default. Unlike some cloud-based StorageClasses (e.g., AWS EBS, GCE PD), which allow resizing with allowVolumeExpansion: true, local storage is constrained by the node’s physical disk.
- Your PVC uses ReadWriteOnce (RWO), tying the PV to a single node. Expansion would require resizing the filesystem on that specific node, which Kubernetes doesn’t manage natively for hostPath.

---

#### **Where to Apply It**
- **Deployment**: Apply cluster-wide from your **workstation** using `kubectl`.
- **Runs On**: The provisioner pod typically runs on a **worker node** (e.g., `worker1` or `worker2`), not the master (due to its `NoSchedule` taint).
- **Storage Location**: Provisions storage on the node where the PVC-using pod is scheduled (usually a worker node).

---

#### **Steps to Apply and Configure**

1. **Verify Cluster Access**
   - From your workstation, confirm you can see all nodes:
     ```bash
     kubectl get nodes
     ```
     Expected output:
     ```
     NAME         STATUS   ROLES    AGE   VERSION
     master-node  Ready    master   1d    v1.29.0
     worker1      Ready    <none>   1d    v1.29.0
     worker2      Ready    <none>   1d    v1.29.0
     ```

2. **Deploy the Local Path Provisioner**
   - Run this from your workstation:
     ```bash
     kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
     ```
   - This installs:
     - `local-path` StorageClass.
     - A Deployment running the `local-path-provisioner` pod in `local-path-storage` namespace.
     - ConfigMaps and RBAC resources.

3. **Check Deployment**
   - Ensure the pod is running (likely on a worker node):
     ```bash
     kubectl get pods -n local-path-storage
     ```
     Example:
     ```
     NAME                              READY   STATUS    RESTARTS   AGE
     local-path-provisioner-xyz        1/1     Running   0          5m
     ```

4. **Set Up Default Storage Path**
   - By default, PVs are created under `/opt/local-path-provisioner` on the node where the pod runs.
   - On each worker node, create and set permissions:
     ```bash
     ssh <worker1-user>@<worker1-ip> "sudo mkdir -p /opt/local-path-provisioner && sudo chmod 777 /opt/local-path-provisioner"
     ssh <worker2-user>@<worker2-ip> "sudo mkdir -p /opt/local-path-provisioner && sudo chmod 777 /opt/local-path-provisioner"
     ```

5. **Change the Default Path (Optional)**
   - To use a custom path (e.g., `/data/storage`):
     - Export the ConfigMap:
       ```bash
       kubectl get configmap local-path-config -n local-path-storage -o yaml > local-path-config.yaml
       ```
     - Edit `local-path-config.yaml`, updating the `paths` field:
       ```yaml
       data:
         config.json: |
           {
             "nodePathMap": [
               {
                 "node": "DEFAULT_PATH_FOR_UNKNOWN_NODES",
                 "paths": ["/data/storage"]
               }
             ]
           }
       ```
     - Apply the change:
       ```bash
       kubectl apply -f local-path-config.yaml
       ```
     - Restart the provisioner pod:
       ```bash
       kubectl delete pod -n local-path-storage -l app=local-path-provisioner
       ```
     - Prepare the new path on worker nodes:
       ```bash
       ssh <worker1-user>@<worker1-ip> "sudo mkdir -p /data/storage && sudo chmod 777 /data/storage"
       ssh <worker2-user>@<worker2-ip> "sudo mkdir -p /data/storage && sudo chmod 777 /data/storage"
       ```

6. **Test the Provisioner**
   - Create a PVC using the `local-path` StorageClass:
     ```yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: test-pvc
     spec:
       accessModes:
         - ReadWriteOnce
       resources:
         requests:
           storage: 1Gi
       storageClassName: local-path
     ```
     Apply it:
     ```bash
     kubectl apply -f pvc.yaml
     ```
   - Verify PV creation:
     ```bash
     kubectl get pv -o wide
     ```
   - Check the path (e.g., `/data/storage/<pvc-uuid>` if customized) on the node listed in the PV’s `NODE` column.

---

#### **Key Configuration Notes**
- **Master Node**: Avoid provisioning storage here; keep it for control plane tasks. The default taint (`node-role.kubernetes.io/master:NoSchedule`) prevents pod scheduling unless removed.
- **Worker Nodes**: Ensure the storage path (default or custom) exists and is writable on `worker1` and `worker2`.
- **Custom Paths per Node**: Modify `nodePathMap` in the ConfigMap to set different paths (e.g., `/data1` for `worker1`, `/data2` for `worker2`).

---

#### **Troubleshooting**
- **Pod Fails**: Check logs:
  ```bash
  kubectl logs -n local-path-storage <provisioner-pod-name>
  ```
- **PVC Pending**: Verify the `local-path` StorageClass (`kubectl get sc`) and path permissions.
- **Wrong Path**: Confirm the ConfigMap reflects your desired path and the pod restarted.

---

#### **Summary**
- Apply the Local Path Provisioner from your workstation with `kubectl` to manage cluster-wide local storage.
- It runs on a worker node and provisions storage where PVC-using pods are scheduled.
- Default path is `/opt/local-path-provisioner`, but you can change it (e.g., to `/data/storage`) by editing the `local-path-config` ConfigMap, restarting the pod, and preparing the new path on worker nodes.
- Test with a PVC to ensure proper setup.

This setup gives you flexible local storage for your cluster, customizable to your needs! Let me know if you need further tweaks.
